{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "\n",
    "The collected data has several issues observed. First, the majority of tweets include pictures or URL links. Second, our data is suffering from an imbalance, where more of our data is classified as negative. The following process below will address the two main issues and prepare the data for modeling. Since the raw data was used for this step, print out statements of the data frame was removed to protect users' privacy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imported Libraries\n",
    "import re\n",
    "import pickle \n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import resample\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing and Feature Engineering: \n",
    "\n",
    "The code below will first load the scraped data into a data frame and fill in any null values from the label column. Feature engineering was conducted by labeling tweets with a website link as positive (1), if 'http' is found in a tweet, and negative (0) if not. The tweet column is then run into a cleaning function that removes URL, hashtags, @(username), \"pic.twitter...\", and non-English characters. \n",
    "\n",
    "**Note:** During the labeling process, tweets that are classified as negative were left blank. Therefore, all blanks were filled with 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read data\n",
    "train = pd.read_csv('../data/raw_training.csv', index_col=0).drop(columns=['user', 'date'])\n",
    "\n",
    "#Fill all missing value with 0 \n",
    "train['label'].fillna(0, inplace=True)\n",
    "\n",
    "#Label tweet with a link (has 'http' in the content) as 1 \n",
    "train['website_link'] = train['tweet'].astype('str').map(lambda x: 1 if 'http' in x else 0)\n",
    "\n",
    "#Function that removes URL, hashtag, @username, non-english characters and \"pic.twitter...\" from tweets. \n",
    "def cleaning(word):\n",
    "    '''Replace URL, hashtags, @useername, non-english characters and pic.twitter with an empty string. \n",
    "    Returns cleaned tweet'''\n",
    "    word = word.lower()\n",
    "    word = re.sub(\"http(\\s+|\\W+|\\w+)+|@(\\s+|\\W+|\\w+)|#(\\w+|\\W+)\", \"\", word) #Removes hashtag, URL, and @username\n",
    "    word = re.sub('(pic.twitter.com.)\\w+', \"\", word) #Removes sentence that starts with pic.twitter\n",
    "    word = re.sub('[^a-zA-z\\s]', \"\", word) #Filter out non-english characters  \n",
    "    return word\n",
    "\n",
    "# Apply function to clean out tweets \n",
    "train['tweet'] = train['tweet'].astype('str').map(cleaning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Addressing Data Imbalance\n",
    "The data is severely imbalanced, with 94% classified as negative (0) and only 6% as positive (1), to resolve the issue, the positive class (minority) will be upsampled with replacement. In contrast, the negative class (majority) will be downsampled. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    0.943392\n",
       "1.0    0.056608\n",
       "Name: label, dtype: float64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['label'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A total of 550 data will be sampled from our dataset, where 300 will come from the positive class, while 250 will be from the negative class. Since our original positive class only has 286 data, an upsampling with replacement will be done to get \"additional\" data. **The defined baseline score is 54%**\n",
    "\n",
    "Code Resource: https://elitedatascience.com/imbalanced-classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0    0.545455\n",
      "0.0    0.454545\n",
      "Name: label, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#Upsampling and Downsampling data \n",
    "majority = train.loc[train['label'] == 0]\n",
    "minority = train.loc[train['label'] == 1]\n",
    "\n",
    "majority_downsample = resample(majority,\n",
    "                              replace=False,\n",
    "                              n_samples=250,\n",
    "                              random_state=42)\n",
    "\n",
    "minority_upsample = resample(minority,\n",
    "                            replace=True,\n",
    "                            n_samples=300,\n",
    "                            random_state=42)\n",
    "\n",
    "#Concatenate minority_upsample and majority_downsample and overwrite train with new data. \n",
    "train = pd.concat([majority_downsample, minority_upsample])\n",
    "print(train['label'].value_counts(normalize=True))\n",
    "\n",
    "#Save new dataset for BERT modeling\n",
    "train.to_csv('../data/train_clean.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing Data and Feature Selection:\n",
    "The cleaned resampled data will be tokenized using a TfidVectorizer. Wherein it removes stopwords and tokenize text into one to four-word combinations (n_grams). Tokenized data will then be fitted in a Logistic Regression model to get tokens' coefficients. Selected features will have an absolute value of coefficient greater than 0.05. The isolated features and vectorizer are pickled to be used in the modeling and prediction steps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenize the tweet column\n",
    "tf = TfidfVectorizer(max_features= 100, stop_words='english', ngram_range=(1,4))\n",
    "Z = tf.fit_transform(train['tweet'])\n",
    "y = train['label']\n",
    "\n",
    "#Fit a Logistic Regression model to tokenize tweets and save coefficient result into a data frame\n",
    "lr_1 = LogisticRegression()\n",
    "lr_1.fit(Z, y)\n",
    "coef_df = pd.DataFrame(lr_1.coef_, columns= tf.get_feature_names()).T\n",
    "\n",
    "#Select only features that have an absolute coefficient > 0.05\n",
    "feature_list = list(coef_df.loc[np.abs(coef_df[0]) > 0.05].sort_values(0, ascending=False).index)\n",
    "\n",
    "#Store tokenize data into a data frame and combine it with cleaned dataset\n",
    "tokens = pd.DataFrame(Z.toarray(), columns=tf.get_feature_names(), index=train.index)\n",
    "train = pd.concat([train, tokens], axis=1)\n",
    "\n",
    "#Save clean tokenize data in csv for modeling \n",
    "train.to_csv('../data/train_clean_token.csv')\n",
    "\n",
    "#Pickle feature list and vectorizer to be use for modeling\n",
    "pd.to_pickle(feature_list, 'pickle/feature.pkl')\n",
    "pd.to_pickle(tf, 'pickle/vectorizer.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note:  Since BERT requires big memory it will be done through google colab. The data cleaned in this notebook will be imported to the google colab to be used for fine-tuning and modeling BERT."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
